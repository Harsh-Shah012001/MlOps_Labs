{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255fb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077b4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaef52de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshah-harsh8\u001b[0m (\u001b[33mshah-harsh8-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd8496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\MS in CS\\MlOps\\Labs\\Lab4\\W&B\\wandb\\run-20251103_194142-fn7atb87</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced/runs/fn7atb87' target=\"_blank\">cnn_plus</a></strong> to <a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced' target=\"_blank\">https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced/runs/fn7atb87' target=\"_blank\">https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced/runs/fn7atb87</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 107ms/step - accuracy: 0.4288 - loss: 1.5940 - val_accuracy: 0.6278 - val_loss: 1.0708 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 119ms/step - accuracy: 0.6484 - loss: 0.9959 - val_accuracy: 0.6591 - val_loss: 0.9781 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 104ms/step - accuracy: 0.7076 - loss: 0.8465 - val_accuracy: 0.6706 - val_loss: 0.9415 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 97ms/step - accuracy: 0.7458 - loss: 0.7274 - val_accuracy: 0.6910 - val_loss: 0.9030 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 92ms/step - accuracy: 0.7799 - loss: 0.6240 - val_accuracy: 0.7034 - val_loss: 0.8760 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▄▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>batch/batch_step</td><td>▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>batch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch/loss</td><td>█▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁</td></tr><tr><td>epoch/accuracy</td><td>▁▅▆▇█</td></tr><tr><td>epoch/epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▄▃▂▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▄▅▇█</td></tr><tr><td>epoch/val_loss</td><td>█▅▃▂▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.77485</td></tr><tr><td>batch/batch_step</td><td>3940</td></tr><tr><td>batch/learning_rate</td><td>0.001</td></tr><tr><td>batch/loss</td><td>0.6388</td></tr><tr><td>epoch/accuracy</td><td>0.7748</td></tr><tr><td>epoch/epoch</td><td>4</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.63896</td></tr><tr><td>epoch/val_accuracy</td><td>0.7034</td></tr><tr><td>epoch/val_loss</td><td>0.87597</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cnn_plus</strong> at: <a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced/runs/fn7atb87' target=\"_blank\">https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced/runs/fn7atb87</a><br> View project at: <a href='https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced' target=\"_blank\">https://wandb.ai/shah-harsh8-northeastern-university/Lab2-CIFAR10-Advanced</a><br>Synced 4 W&B file(s), 10 media file(s), 112 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251103_194142-fn7atb87\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class LogLRCallback(k.callbacks.Callback):\n",
    "    \"\"\"Log optimizer learning rate each epoch.\"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        opt = self.model.optimizer\n",
    "        lr = float(k.backend.get_value(opt.learning_rate))\n",
    "        wandb.log({\"learning_rate\": lr}, step=self.model.optimizer.iterations.numpy())\n",
    "\n",
    "class LogSamplesCallback(k.callbacks.Callback):\n",
    "    \"\"\"Log a small table of predictions + images every epoch.\"\"\"\n",
    "    def __init__(self, x, y, labels, max_rows=16):\n",
    "        super().__init__()\n",
    "        self.x = x[:max_rows]\n",
    "        self.y = y[:max_rows]\n",
    "        self.labels = labels\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.x, verbose=0)\n",
    "        y_true = np.argmax(self.y, axis=1)\n",
    "        y_pred = np.argmax(preds, axis=1)\n",
    "        table = wandb.Table(columns=[\"image\", \"y_true\", \"y_pred\", \"correct\", \"p(y_pred)\"])\n",
    "        for i in range(len(self.x)):\n",
    "            table.add_data(\n",
    "                wandb.Image(self.x[i]),\n",
    "                self.labels[y_true[i]],\n",
    "                self.labels[y_pred[i]],\n",
    "                bool(y_true[i] == y_pred[i]),\n",
    "                float(np.max(preds[i])),\n",
    "            )\n",
    "        wandb.log({f\"samples/epoch_{epoch+1}\": table})\n",
    "\n",
    "class ConfusionMatrixCallback(k.callbacks.Callback):\n",
    "    \"\"\"Log a confusion matrix from the full validation set each epoch.\"\"\"\n",
    "    def __init__(self, x_val, y_val, labels):\n",
    "        super().__init__()\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.labels = labels\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.x_val, verbose=0)\n",
    "        y_true = np.argmax(self.y_val, axis=1)\n",
    "        y_pred = np.argmax(preds, axis=1)\n",
    "        cm_plot = wandb.plot.confusion_matrix(\n",
    "            probs=None, y_true=y_true, preds=y_pred, class_names=self.labels\n",
    "        )\n",
    "        wandb.log({\"confusion_matrix\": cm_plot})\n",
    "\n",
    "\n",
    "# --- Trainer -----------------------------------------------------------------\n",
    "\n",
    "class CIFAR10Trainer:\n",
    "    def __init__(self, project_name=\"Lab2-CIFAR10-Advanced\", run_name=\"cnn_plus\"):\n",
    "        self.cfg = dict(\n",
    "            dropout=0.3,\n",
    "            conv1_filters=32,\n",
    "            conv2_filters=64,\n",
    "            dense_size=128,\n",
    "            learn_rate=0.001,\n",
    "            epochs=5,\n",
    "            batch_size=64,\n",
    "        )\n",
    "\n",
    "        self.run = wandb.init(\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            config=self.cfg,\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "        )\n",
    "        self.config = wandb.config\n",
    "        self.labels = [\n",
    "            \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "            \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "        ]\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        x_train, x_test = x_train.astype(\"float32\")/255.0, x_test.astype(\"float32\")/255.0\n",
    "        y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "        self.X_train, self.X_test = x_train, x_test\n",
    "        self.y_train, self.y_test = y_train, y_test\n",
    "\n",
    "    def _build_model(self):\n",
    "        inputs = k.Input(shape=(32, 32, 3))\n",
    "        x = k.layers.Conv2D(self.config.conv1_filters, (3,3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "        x = k.layers.Conv2D(self.config.conv2_filters, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = k.layers.MaxPooling2D((2,2))(x)\n",
    "        x = k.layers.Dropout(self.config.dropout)(x)\n",
    "        x = k.layers.Flatten()(x)\n",
    "        x = k.layers.Dense(self.config.dense_size, activation=\"relu\")(x)\n",
    "        outputs = k.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "        model = k.Model(inputs, outputs)\n",
    "\n",
    "        opt = k.optimizers.Adam(learning_rate=self.config.learn_rate)\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def _log_model_artifact(self, model):\n",
    "        os.makedirs(\"artifacts\", exist_ok=True)\n",
    "        model_path = \"artifacts/cifar10_model.keras\"\n",
    "        model.save(model_path)\n",
    "        art = wandb.Artifact(\"cifar10_model\", type=\"model\")\n",
    "        art.add_file(model_path)\n",
    "        self.run.log_artifact(art)\n",
    "\n",
    "    def train(self):\n",
    "        model = self._build_model()\n",
    "\n",
    "        callbacks = [\n",
    "            WandbMetricsLogger(log_freq=10),\n",
    "            WandbModelCheckpoint(\"checkpoints/model-{epoch:02d}.keras\", save_weights_only=False),\n",
    "            LogLRCallback(),\n",
    "            LogSamplesCallback(self.X_test, self.y_test, self.labels, max_rows=16),\n",
    "            ConfusionMatrixCallback(self.X_test, self.y_test, self.labels),\n",
    "            k.callbacks.ReduceLROnPlateau(factor=0.5, patience=2, verbose=1),\n",
    "            k.callbacks.EarlyStopping(patience=3, restore_best_weights=True, verbose=1),\n",
    "        ]\n",
    "\n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            epochs=self.config.epochs,\n",
    "            batch_size=self.config.batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        loss, acc = model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        wandb.log({\"final/loss\": loss, \"final/accuracy\": acc})\n",
    "        self._log_model_artifact(model)\n",
    "        self.run.finish()\n",
    "\n",
    "\n",
    "CIFAR10Trainer().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9140351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e83ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
